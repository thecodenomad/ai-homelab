version: '3.8'

services:

#######################
# Experiement with AI #
#######################

#  localai:
#    image: localai/localai:latest-gpu-nvidia-cuda-12-extras
#    container_name: localai
#    ports:
#      - "8080:8080"
#    environment:
#      - NVIDIA_VISIBLE_DEVICES=all
#    volumes:
#      - ./mounts/localai/models:/build/models:Z

##########
# Ollama #
##########

# Just Ollama for API support, no WebUI
#  ollama:
#    image: ollama/ollama:latest
#    ports:
#      - "8080:11434"
#    volumes:
#      - ./mounts/ollama:/root/.ollama:Z
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: 1
#              capabilities: [gpu]
#
#  ollama:
#    image: ghcr.io/open-webui/open-webui:ollama
#    ports: "8080:8080" 
#    volumes:
#      - ./mounts/ollama:/root/.ollama:Z
#      - ./mounts/ollama-data:/app/backend/data:Z
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: 1
#              capabilities: [gpu]    

####################
# Stable Diffusion #
####################

  webui:
    build:
      context: .
      dockerfile: podman/Containerfile
      target: webui
      pull_policy: never
    image: webui:latest
    ports:
      - "7860:7860"
    volumes:
      - ./mounts/models:/app/models:Z
      - ./outputs:/app/outputs:Z
    environment:
      - PYTHONUNBUFFERED=1
      - COMMANDLINE_ARGS=--opt-sdp-attention --xformers --api
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
#        limits:
#          memory: 12g

###########
# ComfyUI #
###########

  comfyui:
    build:
      context: .
      dockerfile: podman/Containerfile
      target: comfyui 
      pull_policy: never
    image: comfyui:latest
    ports:
      - "7861:7861"
    volumes:
#      - ./mounts/models/Stable-diffusion:/app/models/checkpoints:Z
#      - ./mounts/models/VAE:/app/models/vae:Z
#      - ./mounts/models/VAE-approx:/app/models/vae_approx:Z
#      - ./mounts/models/Lora:/app/models/loras:Z
#      - ./mounts/models/hypernetworks:/app/models/hypernetworks:Z
      - ./mounts/models:/opt/models:Z
      - ./mounts/custom_nodes:/opt/custom_nodes:Z
      - ./outputs/comfyui:/app/output:Z
    environment:
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
 #       limits:
 #         memory: 12g    

####################
# Music Generation #
####################

  ace:
    build:
      context: .
      dockerfile: podman/Containerfile
      target: ace 
      pull_policy: never
    image: ace:latest
    ports:
      - "7862:7862"
    volumes:
      - ./mounts/ace/checkpoints:/app/checkpoints:Z
      - ./outputs:/app/outputs:Z
      - ./logs:/app/logs:Z
    environment:
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
#        limits:
#          memory: 12g

